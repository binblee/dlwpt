{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASGD',\n",
       " 'Adadelta',\n",
       " 'Adagrad',\n",
       " 'Adam',\n",
       " 'AdamW',\n",
       " 'Adamax',\n",
       " 'LBFGS',\n",
       " 'NAdam',\n",
       " 'Optimizer',\n",
       " 'RAdam',\n",
       " 'RMSprop',\n",
       " 'Rprop',\n",
       " 'SGD',\n",
       " 'SparseAdam',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_functional',\n",
       " '_multi_tensor',\n",
       " 'lr_scheduler',\n",
       " 'swa_utils']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c_arr = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\n",
    "t_u_arr = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "t_c = torch.tensor(t_c_arr)\n",
    "t_u = torch.tensor(t_u_arr)\n",
    "t_un = 0.1 * t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w, b):\n",
    "    return w * t_u + b\n",
    "\n",
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c)**2\n",
    "    return squared_diffs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-5\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.5483e-01, -8.2600e-04], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_p = model(t_u, *params)\n",
    "loss = loss_fn(t_p, t_c)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, t_u, t_c):\n",
    "    for epoch in range(1, 1+n_epochs):\n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 500 == 0:\n",
    "            print('Epoch %d, loss %f' % (epoch, float(loss)))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, loss 7.860115\n",
      "Epoch 1000, loss 3.828538\n",
      "Epoch 1500, loss 3.092191\n",
      "Epoch 2000, loss 2.957698\n",
      "Epoch 2500, loss 2.933134\n",
      "Epoch 3000, loss 2.928648\n",
      "Epoch 3500, loss 2.927830\n",
      "Epoch 4000, loss 2.927679\n",
      "Epoch 4500, loss 2.927652\n",
      "Epoch 5000, loss 2.927647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012], requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(n_epochs=5000, optimizer=optimizer, params=params, t_u=t_un, t_c=t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.Adam([params], lr=learning_rate)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, loss 24.946461\n",
      "Epoch 1000, loss 15.752771\n",
      "Epoch 1500, loss 9.455151\n",
      "Epoch 2000, loss 5.767542\n",
      "Epoch 2500, loss 3.932838\n",
      "Epoch 3000, loss 3.196811\n",
      "Epoch 3500, loss 2.977151\n",
      "Epoch 4000, loss 2.933181\n",
      "Epoch 4500, loss 2.927968\n",
      "Epoch 5000, loss 2.927654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3660, -17.2952], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(n_epochs=5000, optimizer=optimizer, params=params, t_u=t_un, t_c=t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分割数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,\n",
       " 11,\n",
       " tensor([7, 8, 6, 4, 3, 2, 1, 9, 5]),\n",
       " tensor([ 0, 10]),\n",
       " tensor([ 7,  8,  6,  4,  3,  2,  1,  9,  5,  0, 10]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsamples = t_u.shape[0]\n",
    "n_val = int(0.2 * nsamples)\n",
    "shuffled_indices = torch.randperm(nsamples)\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "n_val, nsamples, train_indices, val_indices, shuffled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t_u = t_u[train_indices]\n",
    "train_t_c = t_c[train_indices]\n",
    "\n",
    "val_t_u = t_u[val_indices]\n",
    "val_t_c = t_c[val_indices]\n",
    "\n",
    "train_t_un = 0.1 * train_t_u\n",
    "val_t_un = 0.1 * val_t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, train_t_u, train_t_c, val_t_u, val_t_c):\n",
    "    for epoch in range(1, 1+n_epochs):\n",
    "        train_t_p = model(train_t_u, *params)\n",
    "        train_loss = loss_fn(t_p = train_t_p, t_c = train_t_c)\n",
    "        val_t_p = model(val_t_u, *params)\n",
    "        val_loss = loss_fn(t_p = val_t_p, t_c=val_t_c)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 500 == 0:\n",
    "            print(f\"Epoch: {epoch}, training loss:{train_loss.item():.4f}, \"\n",
    "                  f\"validation loss: {val_loss.item():.4f}\")\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,\n",
       " tensor([1., 0.], requires_grad=True),\n",
       " 0.01,\n",
       " Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     capturable: False\n",
       "     differentiable: False\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     fused: None\n",
       "     lr: 0.01\n",
       "     maximize: False\n",
       "     weight_decay: 0\n",
       " ))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 5000\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.Adam(params=[params], lr=learning_rate)\n",
    "n_epochs, params, learning_rate, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 500, training loss:22.7140, validation loss: 39.1506\n",
      "Epoch: 1000, training loss:20.4301, validation loss: 35.8703\n",
      "Epoch: 1500, training loss:17.5432, validation loss: 31.6328\n",
      "Epoch: 2000, training loss:14.3864, validation loss: 26.8465\n",
      "Epoch: 2500, training loss:11.2837, validation loss: 21.9186\n",
      "Epoch: 3000, training loss:8.5235, validation loss: 17.2351\n",
      "Epoch: 3500, training loss:6.3116, validation loss: 13.1111\n",
      "Epoch: 4000, training loss:4.7370, validation loss: 9.7477\n",
      "Epoch: 4500, training loss:3.7644, validation loss: 7.2146\n",
      "Epoch: 5000, training loss:3.2619, validation loss: 5.4637\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  0.4840, -14.4444], requires_grad=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_params = training_loop(n_epochs=n_epochs, optimizer=optimizer, params=params,\n",
    "              train_t_u=train_t_u, train_t_c=train_t_c,\n",
    "              val_t_u=val_t_u, val_t_c=val_t_c)\n",
    "result_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, train_t_u, train_t_c, val_t_u, val_t_c):\n",
    "    for epoch in range(1, 1+n_epochs):\n",
    "        train_t_p = model(train_t_u, *params)\n",
    "        train_loss = loss_fn(t_p = train_t_p, t_c = train_t_c)\n",
    "        with torch.no_grad():\n",
    "            val_t_p = model(val_t_u, *params)\n",
    "            val_loss = loss_fn(t_p = val_t_p, t_c=val_t_c)\n",
    "            assert val_loss.requires_grad == False\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 500 == 0:\n",
    "            print(f\"Epoch: {epoch}, training loss:{train_loss.item():.4f}, \"\n",
    "                  f\"validation loss: {val_loss.item():.4f}\")\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,\n",
       " tensor([1., 0.], requires_grad=True),\n",
       " 0.01,\n",
       " Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     capturable: False\n",
       "     differentiable: False\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     fused: None\n",
       "     lr: 0.01\n",
       "     maximize: False\n",
       "     weight_decay: 0\n",
       " ))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 5000\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.Adam(params=[params], lr=learning_rate)\n",
    "n_epochs, params, learning_rate, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 500, training loss:22.7140, validation loss: 39.1506\n",
      "Epoch: 1000, training loss:20.4301, validation loss: 35.8703\n",
      "Epoch: 1500, training loss:17.5432, validation loss: 31.6328\n",
      "Epoch: 2000, training loss:14.3864, validation loss: 26.8465\n",
      "Epoch: 2500, training loss:11.2837, validation loss: 21.9186\n",
      "Epoch: 3000, training loss:8.5235, validation loss: 17.2351\n",
      "Epoch: 3500, training loss:6.3116, validation loss: 13.1111\n",
      "Epoch: 4000, training loss:4.7370, validation loss: 9.7477\n",
      "Epoch: 4500, training loss:3.7644, validation loss: 7.2146\n",
      "Epoch: 5000, training loss:3.2619, validation loss: 5.4637\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  0.4840, -14.4444], requires_grad=True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(n_epochs=n_epochs, optimizer=optimizer, params=params,\n",
    "              train_t_u=train_t_u, train_t_c=train_t_c,\n",
    "              val_t_u=val_t_u, val_t_c=val_t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_forward(t_u, t_c, is_train, params):\n",
    "    with torch.set_grad_enabled(is_train):\n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "    return loss\n",
    "\n",
    "def training_loop(n_epochs, optimizer, params, train_t_u, train_t_c, val_t_u, val_t_c):\n",
    "    for epoch in range(1, 1+n_epochs):\n",
    "        train_loss = calc_forward(t_u=train_t_u, t_c=train_t_c, is_train=True, params=params)\n",
    "        val_loss = calc_forward(t_u=val_t_u, t_c=val_t_c, is_train=False, params=params)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 500 == 0:\n",
    "            print(f\"Epoch: {epoch}, training loss:{train_loss.item():.4f}, \"\n",
    "                  f\"validation loss: {val_loss.item():.4f}\")\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 500, training loss:22.7140, validation loss: 39.1506\n",
      "Epoch: 1000, training loss:20.4301, validation loss: 35.8703\n",
      "Epoch: 1500, training loss:17.5432, validation loss: 31.6328\n",
      "Epoch: 2000, training loss:14.3864, validation loss: 26.8465\n",
      "Epoch: 2500, training loss:11.2837, validation loss: 21.9186\n",
      "Epoch: 3000, training loss:8.5235, validation loss: 17.2351\n",
      "Epoch: 3500, training loss:6.3116, validation loss: 13.1111\n",
      "Epoch: 4000, training loss:4.7370, validation loss: 9.7477\n",
      "Epoch: 4500, training loss:3.7644, validation loss: 7.2146\n",
      "Epoch: 5000, training loss:3.2619, validation loss: 5.4637\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  0.4840, -14.4444], requires_grad=True)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 5000\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.Adam(params=[params], lr=learning_rate)\n",
    "training_loop(n_epochs=n_epochs, optimizer=optimizer, params=params,\n",
    "              train_t_u=train_t_u, train_t_c=train_t_c,\n",
    "              val_t_u=val_t_u, val_t_c=val_t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "练习题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w1, w2, b):\n",
    "    return w1 * t_u ** 2 + w2 * t_u + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 500, training loss:5.1488, validation loss: 10.6766\n",
      "Epoch: 1000, training loss:5.0652, validation loss: 10.4882\n",
      "Epoch: 1500, training loss:4.9475, validation loss: 10.2180\n",
      "Epoch: 2000, training loss:4.7982, validation loss: 9.8662\n",
      "Epoch: 2500, training loss:4.6208, validation loss: 9.4325\n",
      "Epoch: 3000, training loss:4.4211, validation loss: 8.9197\n",
      "Epoch: 3500, training loss:4.2088, validation loss: 8.3371\n",
      "Epoch: 4000, training loss:3.9971, validation loss: 7.7025\n",
      "Epoch: 4500, training loss:3.8020, validation loss: 7.0437\n",
      "Epoch: 5000, training loss:3.6384, validation loss: 6.3973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0051, -0.0557, -1.1935], requires_grad=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 5000\n",
    "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.Adam(params=[params], lr=learning_rate)\n",
    "training_loop(n_epochs=n_epochs, optimizer=optimizer, params=params,\n",
    "              train_t_u=train_t_u, train_t_c=train_t_c,\n",
    "              val_t_u=val_t_u, val_t_c=val_t_c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlwpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
